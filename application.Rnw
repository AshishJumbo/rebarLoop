\section{Estimating Effects in 22 Online Experiments}
\subsection{Data from the ASSISTments TestBed}
We apply and evaluate the methods described in this work to a set of
22 randomized controlled experiments run within the ASSISTments
Testbed. Assistments is a free web-based learning platform used by
real teachers and students for classwork and homework; the system
contains primarily 6th, 7th, and 8th grade mathematics content (as is
the basis of the 22 studies observed in this work), but also hosts
content and users from other domains and grade levels ranging from
early elementary school through college-level.
The Testbed is a platform on which
external researchers can propose and run studies within
ASSISTments, randomizing student and teacher users in real-time into
different software configurations.
The hope is that by A/B testing, researchers can rigorously test
educational or cognitive theories, while simultaneiously guiding the
improvement of ASSISTment's pedegogy.

Once a TestBed proposal is approved, based on IRB and content quality
criteria, its experimental conditions are embedded into an ASSISTments
assignment.
This is then assigned to students, either by a group of
teachers recruited by the researcher or, more commonly,
by the existing population of teachers using ASSISTments in their
classrooms. For example, take an experiment is comparing text-based
hints to video hints.
The proposing researcher would create the alternative hints and embed them
into particular assignable content, otherwise referred to as a ``problem set.''
Then, any
time that a teacher assigns that problem set to his or her students,
those students are randomized to one of the conditions, and, when they
request hints, receive them as either text or video.

There are several types of problem sets that researchers can
utilize when developing their experiments. In the case of the 22
experiments observed in this work, the problem sets are
mastery-based assignments called “skill builders.” As opposed to
more traditional assignments requiring students to complete all
problems assigned, skill builders require students to demonstrate
a sufficient level of understanding in order to complete the
assignment. By default, students must simply answer three
consecutive problems correctly without the use of
computer-provided aid such as hints or scaffolding (i.e. a type of
aid that breaks the problem into smaller steps).
In this way,
completion acts as a measure of knowledge and understanding as
well as persistence and learning, as students will be continuously
given more problems until they are able to reach the completion
threshold.
ASSISTments also includes a “daily limit” of ten
problems to encourage students to seek help if they are struggling
to reach the threshold.
% Both completion and mastery speed,
% calculated as the number of problems needed to complete the skill
% builder (or undefined when the student is unable to complete), are
% common dependent measures used when running experiments in skill
% builders, particularly when such experiments are comparing types
% of content or instructional feedback as is the case in the 22
% randomized controlled experiments observed in this work.

After the completion of a TestBed experiment, the proposing researcher
may download a dataset which includes students' treatment assignments
and their performance within the skill builder, including an indicator
for completion.
Additionally, the dataset includes 30 aggregated features that
describe student performance and activity recorded within the
learning platform prior to random assignment for each respective
experiment.
We combined this data with disaggregated log data from students'
individual prior assignments.

We also gathered analogous data from a large remnant of students who
did not participate in any of the 22 experiments we analyzed.
Rather than use the entire set of past ASSISTments users to build a
remnant, we selected students who resembled those who participated in
the 22 experiments.
Specifically, we first observed the collection of problem sets given
to students in the experiments before being assigned.
The remnant consisted of all other ASSISTments users who had been
assigned to at least one of those assignments.
In other words, the remnant consisted of students who did not
participate in any of the 22 experiments, but had worked on some of
the same content as those who did.
All in all, the remnant consisted of  students.

% \begin{itemize}
%  \item In general
%  \begin{itemize}
%   \item What is TestBed?
%   \item What are skill builders?
%   \item What does ``complete'' mean?
%   \item Which skill builders get experimented on? (researcher chooses, right?)
%   \item Which classrooms participate in the studies? (whoever assigns
%     skill builders with attached experiments, right?)
%  \end{itemize}
%  \item These 22 experiments
%  \begin{itemize}
%   \item What are the interventions under study here?
%   \item What are the subjects of the skill builders? (e.g. Pythagorean
%     theorem, etc. We don't need to list all of them, but maybe give a
%     couple examples and say something in general)
%   \item How were these 22 experiments chosen?
%   \item Sample sizes (see table)
%  \end{itemize}
%  \item The Remnant
%  \begin{itemize}
%   \item What data are available?
%   \item How were subjects chosen?
%   \item Something about lack of overlap between remnant and
%     experiments in both \emph{students} and \emph{skill builders}
%  \end{itemize}
% \end{itemize}

<<loadPackages,include=FALSE,cache=FALSE>>=
library(scales)
library(ggplot2)
library(dplyr)
library(forcats)
library(loop.estimator)
source('code/loop_ols.R')
source('code/loop_ext.R')
@

<<loadData,include=FALSE,cache=FALSE>>=
source('code/dataPrep.r')
@

<<analysisFunctions,include=FALSE,cache=FALSE>>=
source('code/analysisFunctions.r')
@

<<runAnalysis,include=FALSE,cache=TRUE>>=
#fullres <- sapply(levels(dat$problem_set),full,simplify=FALSE)

#save(fullres,file='results/fullres.RData')
@

<<loadResults,include=FALSE,cache=FALSE>>=
load('results/fullres.RData')
rnk <- rank(sapply(fullres,function(x) x['simpDiff','se']))
names(fullres) <- LETTERS[rnk]
dat$ps <- LETTERS[rnk[as.character(dat$problem_set)]]

justSEs <- sapply(fullres,function(x) x[,'se'])
justImp <- round(sapply(fullres,function(x) x[,'improvement'])*100)
@

\begin{table}[ht]
\centering
\begin{tabular}{rlllllllllll}
\hline
<<sampleSize,results='asis',echo=FALSE>>=
ssTab <- table(dat$treatment,dat$ps)
cat('Experiment &',paste(LETTERS[1:11],collapse='&'),'\\\\\n')
cat('\\hline\n')
cat('$n_C$ &',paste(ssTab['0',LETTERS[1:11]],collapse='&'),'\\\\\n')
cat('$n_T$ &',paste(ssTab['1',LETTERS[1:11]],collapse='&'),'\\\\\n')
cat('\\hline\n')
cat('Experiment &',paste(LETTERS[12:22],collapse='&'),'\\\\\n')
cat('\\hline\n')
cat('$n_C$ &',paste(ssTab['0',LETTERS[12:22]],collapse='&'),'\\\\\n')
cat('$n_T$ &',paste(ssTab['1',LETTERS[12:22]],collapse='&'),'\\\\\n')
@
\hline
\end{tabular}
\caption{Sample Sizes for each of the 22 TestBed A/B Tests}
\label{tab:sampleSizes}
\end{table}

\subsection{Deep Learning in the Remnant to Impute
  Completion}\label{sec:deepLearning}

We used the remnant to train a variant of a recurrent neural network \cite{} called
Long-Short Term Memory (LSTM) networks \cite{}---a ``deep
learning'' algorithm---to
predict students' assignment completion.
Unfortunately, we were undable to identify a large number of instances
in the remnant of students working on the same skill builders as were
in the 22 experiments; instead, we trained the LSTM model to predict a
student's completion on whatever skill builder he or she worked on
next.
Specifically, we considered sequences of at most ten worked skill
builders within each student's history, and attempted to predict that
student's completion on an 11th skill builder.

Deep learning models, and particularly LSTM networks, have been previously applied
successfully to model similar temporal relationships \cite{}. % Such models have been applied in areas of
% pattern recognition /cite and natural language processing /cite, but
% have also been applied within educational contexts to model such
% student attributes as knowledge and affect while interacting with
% computer-based learning platforms /cite.
Deep neural networks are essentially iterated generalized linear
models: a set of outcomes are modeled as a function of a linear
combination of latent ``hidden'' variables, which are themselves
functions of previous layers of hidden variables.
The process iterates until a bottom layer of hidden variables, which
is a function of observed covariates.
The LSTM model extends this logic to panel data: in each time step,
the model combines information from the current observed time step
with an aggregation of previous hidden layer outputs as well as an
internal ``cell memory'' to best inform the model’s outcome
estimates.

More precisely,
the model is represented as several fully-connected layers, with a set
of inputs feeding into one or more hidden layers, and then to an
output layer corresponding with the observed dependent measures; this
results in an $n\times m$ matrix of weights between layers
corresponding to $n$ nodes in a layer and $m$ nodes in the subsequent
layer.
A nonlinear function is then commonly applied to the output of each
layer; we apply a hyperbolic tangent (tanh) function to the output of
the LSTM layer and a sigmoid function to the estimates produced by the
output layer.
We used 16 covariates to describe each single time step (representing
a student’s performance on a single assignment), which then feeds into
a hidden LSTM layer of 100 values, or units, which is used to inform
an output layer of two units corresponding with two outcomes of
interest: completion and inverse mastery speed---a continuous variable
that equals the reciprical of the number of problems a student worked,
if they completed the assignment, and zero otherwise.
Using the LSTM network to predict two outcomes is an example of
multi-task learning \cite{}, which attempts to
reduce model overfitting by simultaneously observing multiple
dependent measures, regularizing the model. Completion inverse mastery
speed together represent two different measures of student
performance; including both prevents the model from overfitting to any
one measure.

% Input Feature
% Description
% Problems started
% The number of problems started by the student
% Problems completed
% The number of problems completed by the student
% Inverse mastery speed
% 1 divided by the number of problems needed to complete the skill builder assignment, or 0 where the student did not complete
% completion
% Whether or not the assignment was completed by the student
% Root problems started
% The square root of the number of problems started
% Root problems completed
% The square root of the number of problems completed
% Root inverse mastery speed
% The square root of inverse mastery speed as defined above
% Percent correctness
% The percentage of problems answered correctly on the first attempt
% Root percent correctness
% The square root of the percentage of problems answered correctly on the first attempt
% Average attempts
% The average number of attempts to answer each problem
% Root Average Attempts
% The square root of average attempts to answer each problem
% Average first response time
% The average time taken per problem before taking the first action
% Average problem duration
% The average time, in seconds, needed to solve each problem
% Average days working
% The average number of distinct days a student worked on problems (to identify students who may leave an assignment and come back the next day)
% Average attempt first
% The percentage of problems for which the first action was an attempt (as opposed to a help request)
% Average bottom-out hint usage
% The percentage of problems for which a student needed to be given the answer

% Due to the large number of learned parameters and high complexity of
% these models, they are often able to take advantage of large datasets
% to learn non-linear relationships not only between covariates and
% dependent measures within a single time step, but also can learn
% non-linear temporal relationships (e.g. the likely diminishing impact
% of performance on earlier assignments over time). Another affordance
% of these models is through existing developed techniques such as
% dropout and multi-task learning. Dropout \cite is a model training
% technique that randomly omits and reintroduces model parameters at
% each training step in an effort to prevent overfitting by reducing
% model dependence on individual covariates or constructed features
% within the model.
During training, the model uses an adaptive gradient
descent method called Adam, optimizing model weights to minimize a
simple sum of binary cross entropy loss and root mean squared error
for the outcomes of completion and inverse mastery speed
respectively.
The training procedure involves the iterative update of model weights
through gradient descent until a stopping criterion is met; in our
case performance on a holdout set. Specifically, we used 30\% of the
training set to estimate the point at which when a 5-epoch moving
average of calculated
model error on this holdout set either plateaus (i.e. the difference
in performance drops below a small threshold) or begins to increase,
signifying overfitting; the use of a moving average helps to prevent
the model from stopping too early due to small fluctuations in the
difference of model error from one epoch to the next.
We specified the LSTM model's hyperparameters
based on previously successful model structures and training
procedures within the context of education.
We evaluate the model using a 10-fold cross validation to gain a
measure of model fit (leading to an ROC area under the curve of 0.82
and root mean squared error of 0.34 for the dependent measure of next
assignment completion) before then training the model on the full set
of remnant data.

We then gave the trained model the sequences of assignment
performances of students in the experimental set to gain an estimate
of experiment completion for each student across each of the 22
experiments.





% \begin{itemize}
%  \item General structure: predicts what as a function of what?
%  \item briefly describe deep learning
%  \item describe a couple modeling choices
%  \item how was the model evaluated? (cross validation?)
% \end{itemize}
\subsection{Results}

In each of the 22 experiments, we calculated five different unbiased
ATE estimates:
\begin{enumerate}
 \item the simple difference \eqref{eq:tauSD}
\end{enumerate}
Two estimates using Deep Learning predictions from the remnant:
\begin{enumerate}
\setcounter{enumi}{2}
 \item rebar, i.e. the simple difference estimate with
    $Y-\pred$ replacing outcomes $Y$
 \item ReLOOP*, i.e. LOOP-OLS with $\pred$ as the only covariate
\end{enumerate}
Two estimates using TestBed covariates:
\begin{enumerate}
\setcounter{enumi}{4}
 \item LOOP using only covariates supplied within TestBed
 \item ReLOOP, using both $\pred$ and provided TestBed covariates
\end{enumerate}
Since each of these estimates is unbiased, we will focus on the
estimated standard errors.

\subsubsection{Simple Difference, Rebar, and LOOP}

<<makeGraphics,include=FALSE>>=
source('code/graphics.r')
@

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{ses1.pdf}
\caption{Standard errors for four ATE estimates in each of the 22
  ASSISTments online experiments, in percentage point units. The experiments are ordered
  according to the standard error of the simple difference estimate.}
\label{fig:ses1}
\end{figure}

Figure \ref{fig:ses1} displays estimated standard errors from the
simple difference estimator, rebar, and ReLOOP*.

<<rebar,include=FALSE>>=
best <- colnames(justImp)[which.max(justImp['rebar',])]
badRebar1 <- colnames(justImp)[which.min(justImp['rebar',])]
ji <- justImp[,-which.min(justImp['rebar',])]
badRebar2 <- colnames(ji)[which.min(ji['rebar',])]
@

<<badReLOOP,include=FALSE>>=
badReLOOP1 <- -justImp['strat1',justSEs['strat1',]>justSEs['simpDiff',]]
badReLOOP <- -justImp['strat3',justSEs['strat3',]>justSEs['simpDiff',]]
@
%Within-sample covariance adjustment via LOOP improved precision in \Sexpr{sum(justSEs['justCovs',]<justSEs['simpDiff',])}

Rebar standard errors were lower than their simple difference
counterparts in \Sexpr{sum(justSEs['rebar',]<justSEs['simpDiff',])} of
the 22 experiments.
Notably, in one case (labeled experiment
``\Sexpr{best}'') rebar
reduced the simple difference standard error by approximately
\Sexpr{justImp['rebar',best]}\%.
On the other hand, in
\Sexpr{sum(justSEs['rebar',]>justSEs['simpDiff',])} experiments, rebar
increased the standard error, most egregiously in experiment%s
\Sexpr{badRebar1}% and \Sexpr{badRebar2}
, in which rebar increased
standard errors by a factor of
\Sexpr{justImp['rebar',badRebar1]}\%. % and
%\Sexpr{justImp['rebar',badRebar2]}\%, respectively.
In these cases, apparently, the imputations from the model fit to the
remnant were particularly inaccurate.
Because experimental outcomes played no role whatsoever in rebar
covariate adjustment, the adjustment was blind to this inaccuracy, and
was unable to anticipate the resulting increase in standard errors in
those cases.

In contrast, the ReLOOP* estimator incorporates information on
imputation accuracy into its covariate adjustment.
Across the board, ReLOOP* standard errors were smaller or roughly equal to
simple difference standard errors.
(In \Sexpr{sum(justSEs['strat1',]>justSEs['simpDiff',])} cases ReLOOP*
standard errors are very slightly larger, a factor of about \Sexpr{max(badReLOOP)}\%.)
In those cases in which rebar performed well, ReLOOP* tended to
perform even better.
For instance, in experiment \Sexpr{best}, ReLOOP* reduced the simple
difference standard errors by a factor of \Sexpr{justImp['strat1',best]}\%.



\subsubsection{Incorporating Standard Covariates}
\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{ses2.pdf}
\caption{Standard errors for three ATE estimates in each of the 22
  ASSISTments online experiments, in percentage point units. The experiments are ordered
  according to the standard error of the simple difference estimate.}
\label{fig:ses2}
\end{figure}


<<reloop,include=FALSE>>=
bestL <- colnames(justImp)[which.max(justImp['justCovs',])]
stopifnot(sum(justSEs['justCovs',]>justSEs['simpDiff',])==4)
@


Figure \ref{fig:ses2} shows standard errors for effect estimates that
incorporate within-sample covariate adjustment via LOOP, for ReLOOP
effect estimates that additionally incorporate remnant-based
predictions, and for simple difference estimates.
In most cases, the covariate adjustment in LOOP reduced standard
errors.
In some cases the reduction was substantial---in experiment
\Sexpr{bestL}, LOOP reduced standard errors by a factor of
\Sexpr{justImp[,bestL]}\%.
In four cases LOOP standard errors were slightly greater than simple
difference, though the difference was moderate or
small---\Sexpr{-min(justImp['justCovs',])}\% or less.

Across all 22 experiments, the standard errors for the ReLOOP
estimates were lower or roughly equal to standard errors for
the other estimates---in these datasets, ReLOOP indeed appears to have
incorporated the advantages of its constituent methods.
These gains in precision can have substantial practical benefit.
Figure \ref{fig:ssMult} expresses changes in standard errors in terms
of sample size---if the simple difference standard error is
$SE_{sd}$ and the ReLOOP standard error is $SE_{rl}$, then covariate
adjustment via ReLOOP is roughly equivalent to multiplying the sample
size by $SE_{sd}^2/SE_{rl}^2$.
The increases in precision due to ReLOOP were, in some cases,
equivalent to increasing the sample size by 50\% or more, and in one
case increasing the sample size by nearly 300\%.


\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{ssMult.pdf}
\caption{Increases in sample size equivalent to changes in standard
  error due to ReLOOP. That is, $SE_{sd}^2/SE_{rl}^2$, where $SE_{sd}$
  is the simple difference standard error and $SE_{rl}$ the ReLOOP standard error. }
\label{fig:ssMult}
\end{figure}



