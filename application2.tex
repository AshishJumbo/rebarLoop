\section{Estimating Effects in a School-Level Effectiveness Trial}
In a second illustration of ReLOOP, we conducted a secondary data
analysis of a randomized trial of the Cognitive Tutor Algebra 1 (CTA1)
curriculum, first reported in \citet{ctaiEval}.
The Cognitive Tutor is a pioneer ``intelligent tutoring system,''
a computer program designed to teach users, alongside teachers.
The CTAI curriculum, created by Carnegie Learning, combines the
Cognitive Tutor for Algebra I with a student-centered classroom
curriculum.

The CTA1 effectiveness study was designed to measure CTA1's effect on
students' scores on an Algebra I posttest, in a wide variety of
schools under realistic conditions.
The study took place over the course of two academic years,
20007--2008 and 2008--2009, in middle schools and high schools across
seven states.
The study found substantial positive effects in the 2008--2009 high
school sample, but not in the other strata.

The design of the study was a blocked, cluster-randomized trial:
schools were paired within districts, and randomized to use either
CTA1 or business as usual.
Only students taking Algebra I were included in the analysis sample.

Our secondary analysis uses aggregated school-level data from schools
in Texas, were school-level data are readily accessible through the
Academic Excellence Indicator
System (AEIS\footnote{\url{https://rptsvr1.tea.texas.gov/perfreport/aeis/index.html},
  Accessed 3/4/2019.}).
We estimated the effect of a school's assignment to the CTA1 condition
on the proportion of students achieving satisfactory performance on
the math portion of the Texas Assessment of Knowledge and Skills
(TAKS), a statewide standardized test.
We conducted separate analyses for middle school and high school, in
the two study years,
using passign rates on the 8th grade and 9th grade TAKS, in 2008 and 2009
respectively.

\subsection{Data}




Sixteen pairs of middle schools and six pairs of high schools were
randomized in the Texas arm of the study.
The original evaluation used student level data, our secondary
analysis uses only school-level data, so for our purposes $n_T=n_C=16$
in the middle school stratum and $n_T=n_C=6$ in the high school
stratum, a small sample size.

Outcomes (TAKS passing rates) and covariates were all downloaded from
the Texas AEIS.
We gathered a total of 3568 covariates for middle schools
and 4323 for high schools.
These included aggregate student demographics and academic
performance, alongside staff and financial data, from school years
beginning in 2003--2006, as well as student demographic, staff, and
financial data from the 2007--2008 school year, the first year of the
study.
The large number of covariates is due, in large part, to the
aggregation: data include, for instance, the number, percentage, and
academic performance for students of several demographic subgroups.
We singled out one of these covariates as particuarly promising: the
school-level ``pretest,'' i.e. the TAKS passing rate from 2006--2007,
the year before the onset of the CTA1 experiment.

The remnant consisted of Texas schools with observed outcome data,
for the most part middle schools and high schools, which
did not participate in the CTA1 study.
All in all, the remnant consisted of 1804 middle schools and
1548 high schools.

We used the remnant to fit random forests \citep{randomForest} using
the fast implementation from the \texttt{R} package \texttt{ranger},
to impute potential
TAKS passing rates for high schools and middle schools in the two
school years.
To fit each model, we regressed school-level TAKS mathematics passing
rates in each remnant on all school level covariates.
Out-of-bag $R^2$ values were roughly 0.7 for all models.

\subsection{Results}

Two modifications to the LOOP algorithm were necessary, since the randomization in the
CTA1 study was paired, not Bernoulli.
First, rather than leave out each individual observation when fitting
the imputation model, we left out each randomization pair, since
randomization was independent between pair.
Second, we estimated standard errors for all estimates using the
design-based standard error estimate
\citep[e.g.][]{gadbury2001randomization}--$s(\bm{d})/\sqrt{n_p}$,
where $s(\cdot)$ is the sample standard deviation, $\bm{d}$ is a
vector of within-pair differences in outcomes (simple difference
estimator), prediction errors (rebar), or $y-\hat{m}$ (LOOP and
ReLOOP).
A third modification to LOOP was due to the small sample sizes: rather than
using random forests to impute potential outcomes based on covariates
within the sample, we used OLS, and used school pretest as the sole
within-sample covariate.

% latex table generated in R 3.4.4 by xtable 1.8-2 package
% Wed Mar  6 07:45:22 2019
\begin{table}[ht]
\centering
\begin{tabular}{rll|ll}
   &\multicolumn{2}{c}{2007--2008}&\multicolumn{2}{c}{2008--2009}\\ 
 &Estimate&SE&Estimate&SE\\ 
 \hline
Simple Difference & -6.82 & 2.22 & -3.85 & 2.10 \\ 
  Rebar & -3.66 & 1.49 & -0.94 & 1.58 \\ 
  LOOP & -2.02 & 1.61 & 1.28 & 1.91 \\ 
  ReLOOP* & -3.51 & 1.49 & -0.92 & 1.58 \\ 
  ReLOOP & -2.19 & 1.48 & 1.05 & 1.77 \\ 
   \hline
\end{tabular}
\caption{Estimates and standard errors of assignment to the CTA1 condition on 8th or 9th grade Texas state tests passing rates in the two study years. The middle- and high-school samples were pooled} 
\label{tab:resultsCT}
\end{table}


The results are displayed in Table \ref{tab:resultsCT}.
For simplicity, we pooled results within each study year.
In 2007--2008, all four covariance adjustment techniques reduced the
standard error substantially. Compared with the simple difference
standard error of 2.2 percentage points,
the rebar standard error of 1.5
represented an improvement of 33\%, and
the ReLOOP standard error of 1.5
represented an improvement of 148\%.

In the following school year, rebar peformed similarly, giving a
standard error of 1.6 percentage points, and
158\% improvement over the the simple
difference standard error of 2.1.
In contrast, LOOP---using only the pretest as a covariate---only
improved on the simple difference standard error by
9\%.
Surprisingly, ReLOOP, using both predictions from the remnant and
school pretests, did not perform as well as rebar, improving the
simple difference standard error by merely
16\%.
This is likely due to the relative unreliability of OLS in small samples.
