\subsection{Causal Inference from Experiments}
Consider a randomized experiment to estimate the average
effect of a binary treatment $T$ on an outcome $Y$.
Following \citet{neyman:1923} and \citet{rubin1974estimating}, for subject $i=1,\dots,N$,
let potential outcomes $\yti$ and $\yci$ represent the outcome value
$Y_i$ that $i$ would have exhibited if he or she had (perhaps
counterfactually) been assigned to treatment, $T_i=1$ or control,
$T_i=0$.
Observed outcomes are a function of treatment assignment and
potential outcomes:
\begin{equation*}
  Y_i=T_i\yti+(1-T_i)\yci
\end{equation*}
Then define the treatment effect for $i$ as $\tau_i=\yti-\yci$; our goal will be
to estimate the average treatment effect (ATE),
$\tbar\equiv\sum_i\tau_i/N=\bar{y}_T-\bar{y}_C$, where
$\bar{y}_T=\sum_T{\yti}/N$ is the mean of $y_T$ over all $N$ units in
the experiment and $\bar{y}_C$ is defined similarly.

If both $\yci $ and $\yti $ were known for each subject $i$,
statistical modeling would be unnecessary---researchers could
calculate $\tbar $ exactly, without error, % as the
%difference between $\bar{\yt}=\sum_i\yti/N$ and
%$\bar{\yc}=\sum_i\yci/N$, or equivalently
by simply averaging observed $\tau$.
In practice, we never observe both $\yci$ and $\yti$.
Instead, we rely on the experimental setup to estimate and infer
causation.
Since the treatment and control groups are each random samples of
the $N$ participants, survey sampling literature provides design-based
unbiased estimators of $\bar{y}_T$ and $\bar{y}_C$ based on observed $Y$
and the known distribution of $T$.
These estimators, and their associated inference, depend only on the
experimental design, and not on modeling assumptions.
The survey sample structure of randomized experiments allows us to
infer counterfactual potential outcomes (at least on average) and estimate $\tbar$ as if
$\tau_i$ were available for each $i$, albeit with sampling error.

Let $M_i=T_i\yci+(1-T_i)\yti$, $i$'s unobserved
counterfactual outcome---when $i$ is treated, $M_i=\yci$ and when $i$
is in the control condition $M_i=\yti$.
Then $i$'s treatment effect may be expressed as $\tau_i=Y_i-M_i$ if
$i$ is in the treatment group or $\tau_i=M_i-Y_i$ if $i$ is in the
control group.
Although $M_i$ is, by definition, unobserved, it plays a central role in
causal inference, as does its expectation,
\begin{equation*}
m_i\equiv p\yci+(1-p)\yti
\end{equation*}
which will play a prominent role in the method we are proposing.

We will use this framework to analyze the 22 TestBed experiments.
These are examples of ``Bernoulli experiments,'' in
which each $T_i$ is an
independent Bernoulli trial: $Pr(T_i=1)\equiv p$, with $0<p<1$, and
$T_i\independent T_j \text{ if } i\ne j$.
In the TestBed experiments,
$p=1/2$.
Then, estimation and inference about $\bar{\tau}$ is based on
the observed values of $Y$ and $T$, and the known value of $p$.
Let
\begin{equation*}
U_i=\begin{cases}
\frac{1}{p} & T_i=1\\
-\frac{1}{1-p} & T_i=0
\end{cases}
\end{equation*}
be subject $i$'s signed inverse probability weights.
Note that $\EE U_i=0$, and $\EE U_iY_i=\tau_i$.
(To see this, note that when $T=1$, with probability $p$,
$Y_i=\yti$ and $U_iY_i=\yti/p$; when $T=0$, with probability
$1-p$, $U_iY_i=-yci/(1-p)$).
Then $U_iY_i$ may be thought of as an unbiased estimate of $\tau_i$, and $\thipw=\sum_iU_iY_i/N$ is an unbiased estimate
of $\bar{\tau}$.
In fact, $\thipw$ is identical to the ``Horvitz-Thompson'' estimator
of, e.g., \citet{aronowMiddleton}
\begin{equation*}
\thipw=\frac{1}{N}\displaystyle\sum_{i\in\mathcal{T}}
\frac{Y_i}{p}-\frac{1}{N}\displaystyle\sum_{i\in\mathcal{C}}\frac{Y_i}{1-p}
\end{equation*}
where $\mathcal{C} = \{i | T_i = 0\}$ is the control group and
$\mathcal{T} = \{i | T_i = 1\}$ is the treatment group.
This, in turn, is the difference between the Horvitz-Thomson
estimates of $\bar{\yt}$ and $\bar{\yc}$ \citep{horvitzThompson}.

\sloppy
The sampling variance of $\thipw$ proceeds from the same principals:
the variance of $U_iY_i$ is
\begin{equation}\label{eq:varTauHat1}
V(U_iY_i)=\left(\yti\sqrt{\frac{1-p}{p}}+\yci\sqrt{\frac{p}{1-p}}\right)^2=\frac{m_i^2}{p(1-p)}
\end{equation}
and, since  subjects' treatment assignments are mutually independent,
$V(\thipw)=1/N^2\sum_im_i^2/\{p(1-p)\}$.
Since $\yti $ and $\yci $ are never simultaneously observed,
$V(\thipw)$ is not identified; however, it may be bounded in expectation,
as $\hat{V}(\thipw)=\sum_iU_i^2Y_i^2/N^2$:
$\EE\hat{V}(\thipw)\ge V(\thipw)$.
(See \citealt{aronowMiddleton} for equivalent expressions for more
general experimental designs.)

Strangely, $\thipw$ and $\hat{V}(\thipw)$ are not translation-idenpendent;
i.e. adding a constant to each $Y$ changes their values without
changing the estimand $\bar{\tau}$.
The more popular ``simple difference'' estimator \citep{neyman:1923},
\begin{equation}\label{eq:tauSD}
\hat{\tau}^{SD} = \frac{1}{n_T}\sum_{i \in \mathcal{T}}Y_i - \frac{1}{n_C}\sum_{i \in \mathcal{C}}Y_i=\bar{Y}_{T=1}-\bar{Y}_{T=0}
\end{equation}
and its associated variance estimator
$s^2(\bm{Y})_{T=0}/n_C+s^2(\bm{Y})_{T=1}/n_T$, where
$s^2(\bm{Y})_{T=0}$ is the sample variance of control outcomes
%$\sum_{i\in\mathcal{C}}(Y_i-\bar{Y}_{T=0})^2/(n_C-1)$
and
$s^2(\bm{Y})_{T=1}$ is defined similarly, do not have this property.
Our presentation here focuses on $\thipw$ as a jumping-off point for
subsequent methodological development;
$\tau^{SD}$ will will play a smaller role.


Classical survey sampling theory implies that $\thipw$ is
asymptotically normal, with asymptotic variance of at most
$\hat{V}(\thipw)$, so Wald-type confidence intervals of the form
$\thipw\pm z_{\alpha/2}\hat{V}(\thipw)^{1/2}$ achieve at least nominal coverage
in large samples.
These guarantees hold regardless of the distribution of
$\{\yc,\yt\}$---they depend only on the experimental design.



\subsection{Design-Based Covariate Adjustment}

The reason for error in estimating $\hat{\tau}$, is our inability to observe counterfactual
potential outcomes $M$.
As we've seen, randomized trials, coupled with design-based estimators
like $\thipw$, use comparison groups and survey sampling theory to
fill in this missing information.
Baseline covariates---a vector $\bm{x}_i$ of data for subject $i$
gathered prior to treatment randomization---may provide an alternative
strategy.
To see how, say a researcher had constructed algorithms $\predcx$ and
$\predtx$ designed to impute $\yc$ and $\yt$, respectively, from
$\bm{x}$.
Then, if $\hat{M}_i$ is an imputation of $i$'s missing counterfactual,
 $\predcx$ or $\predtx$, then they may estimate $\tau_i$ as $Y_i-\hat{M}_i$ (if $T_i=1$) or $\hat{M}_i-Y_i$ (if $T_i=0$).
In general, the bias of algorithms such as $\predc$ and
$\predt$, will be unknown, so these effect estimates may be inadvisable.
On the other hand, imperfect or potentially biased imputations of
potential outcomes can, \emph{when combined with randomization}, yield
substantial benefits.

The approach we will take to combining covariate adjustment with
randomization follows \citet{loop}, as will its presentation here.
It has antecedents in \citet{rosenbaum2002covariance},
\citet{aronowMiddleton}, \citet{wager2016high}, and \citet{tame}.
In a Bernoulli experiment, note that
\begin{align*}
U_i(Y_i-m_i)&\\
&=\begin{cases}
\frac{1}{p}(\yti-p\yci-(1-p)\yti)&T_i=1\\
-\frac{1}{1-p}(\yci-p\yci-(1-p)\yti)&T_i=0 \end{cases}\\
&=\begin{cases}
\frac{p(\yti-\yci)}{p}&T_i=1\\
\frac{(1-p)(\yti-\yci)}{1-p}&T_i=0
\end{cases}\\
&=\tau_i
\end{align*}.
This suggests using imputations $\predcx$ and $\predtx$, to estimate $m_i$ as
$\hmi$,
and estimating $\tau_i$ as
\begin{equation*}
\thim\equiv U_i(Y_i-\hmi)
\end{equation*}

It turns out that $\thim$ is unbiased if algorithms $\predc$ and $\predt$
are constructed in such a way that
\begin{equation}\label{eq:indPred}
\{\predcx,\predtx\}\independent T_i.
\end{equation}
Since $\bm{x}_i\independent T_i$ by design, (\ref{eq:indPred}) is
tantamount to requiring that $T_i$, and variables such as $Y_i$ that
depend on it, play no role in constructing algorithms $\predc$ and
$\predt$.
%This requirement will be the focus of this paper's contributions.

Under (\ref{eq:indPred}), $\thim$ is indeed unbiased:
\begin{equation}
\EE \thim=\EE U_iY_i+\EE U_i\hmi=\EE U_iY_i+\EE U_i\EE\hmi=\EE
U_iY_i=\tau_i
\end{equation}
where we use the facts that $\EE U_i=0$ and $\EE U_iY_i=\tau_i$.
Finally, define ATE estimate
\begin{equation} \label{eq:thm}
\thm=\frac{1}{N}\displaystyle\sum_{i=1}^N
\thim=\frac{1}{N}\displaystyle\sum_{i=1}^N
\frac{T_i(Y_i-\hmi)}{p}-\frac{(1-T_i)(Y_i-\hmi)}{1-p}
\end{equation}
The unbiasedness of $\thm$ for $\bar{\tau}$ follows from the
unbiasedness of each of its summands, $\thim$ for $\tau_i$.

Crucially, this unbiasedness holds even if $\predcx$ and $\predtx$ are biased; algorithms $\predc$ and $\predt$ need not be unbiased, consistent, or correct in any sense.
As long as $\predcx$ and $\predtx$ are constructed to be independent
of $T_i$, $\thim$ will be unbiased.
The same cannot be said for regression-based covariate
adjustment, the common technique of regressing $Y$ on $T$ and $\bm{x}$
\citep{freedman2008regression}.

The goal of the covariate adjustment in $\thim$ is to estimate average
effects with greater precision;
its success in this regard depends on the predictive accuracy of $\predcx$ and $\predtx$.
\citet{loop} show that
\begin{equation}
V(\thim|\hmi)=\frac{(\hmi-m_i)^2}{p(1-p)}
\end{equation}
Accurate imputations of $\yci$ and $\yti$, and hence of $\hmi$, yield precise estimation of $\tau_i$.
On the other hand, inaccurate imputations (such that
$(\hmi-m_i)^2>m_i^2$) will decrease precision---though, again, without
causing bias.
%Note, however, that predictive accuracy of $\predc$ or $\predt$ does not impact bias.
The sampling variance of $\thm$ depends on the dependence structure of
$\hat{m}$, and will be discussed in the following two sections.

Under this framework, successful covariate adjustment
requires imputations $\predcx $ and $\predtx $ that are accurate and
independent of $T$.
To satisfy the independence condition, $i$'s outcome $Y_i$, which is a
function of $T$, cannot play a role in the construction of the algorithms $\predc$ and
$\predt$; they must be trained using other data.
Recent literature proposes two solutions to this problem.
 One approach \citep{rebarEDM} suggests
estimating algorithm $\predc $ for all participants in the
experiment using the remnant.
A second approach \citep{loop} trains a separate algorithm for each
experimental participant $i$, using data from experimental units other
than $i$.
The following two subsections will review these two approaches in some
depth.
The remainder of the paper will discuss their combination.

\subsection{Rebar: Covariate Adjustment Using the Remnant}\label{sec:intro.remant}

Modern field trials are often conducted within a very data-rich
context, in which high-dimensional and rich covariate data is
automatically, or already, collected for all experiment participants.
For instance, in the TestBed experiments, system administrators have
access to log data for every problem and skill builder each
participating student worked before the onset of the experiment.
In other contexts, such as healthcare or education, rich
administrative data is available.
In fact, these covariates are available for a much wider population
than just the experimental participants---in the TestBed case, there
is log data for all ASSISTments users.
In healthcare or education examples, administrative data is available
for every student or patient in the system, not just for those who
were randomized to a treatment or control condition.
Often, as in the TestBed case, the outcome variable $Y$ is also drawn from administrative or
log data.
We refer to subjects within the same data system in which the
experiment took place---i.e. for whom covariate and outcome data are
available---but who were not part of the experiment, as the ``remnant'' from the experiment.
The remnant from a TestBed experiment consists of all ASSISTments
users for whom log data is available but who did not participate in
the experiment.

Clearly, pooling data from the remnant with data from the experiment
undermines the benefits and justification for randomization.
On the other hand, \citet{rebarEDM} argues that data from the remnant
can play a role in covariate adjustment.
When an RCT contrasts an experimental condition in $\mathcal{T}$ with
business as usual in $\mathcal{C}$, then only the control condition
will be present in the remnant.
Then, an analyst may train an algorithm $\predcr{\cdot}$ to data
from the remnant, and use the trained algorithm, in conjunction with
experimental participants' own covariates $\bm{x}$, to impute their
control potential outcomes as $\predcxr\equiv \predcr{bm{x}_i}$.
Specifically, let $\hmi^{rebar} \equiv \predcxr$, and
use $\hmi^{rebar}$ to construct effect estimators $\thim$ and
$\thm$.
\citet{rebarEDM} calls this method ``remnant-based residualization,''
or ``rebar.''

Since the algorithm $\predcr{\cdot} $ is trained using data from a
separate sample from the experiment, and $\bm{x}\independent T$ by
design, imputations $\predcxr $ satisfy the
independence criterion (\ref{eq:indPred}).
In fact, $\tilde{x}_{i}$ may be treated formally just like any other
covariate---a fact which is reflected in its notation.
Furthermore, $\predcr{\cdot}$ may be trained and assessed in any way, as
long as only remnant data is used.
This process can be iterative, so that an analyst may train a candidate
model, assess its performance (perhaps with $k-$fold
cross-validation), modify the algorighm, and repeat until suitable
performance is achieved.
This follows from the fact that inference proceeds from the
randomization of $T$, and algorithms trained in the remnant are invariant to
$T$.
Any realization of the assignment vector $\bm{T}$ would have given
rise to precisely the same imputations $\predcxr $.
The frequent problem of post-selection inference, which is exacerbated
when the dimension of $\bm{x}$ is large, does not apply here.

\citet{rebarEDM} used rebar to analyze the 22 TestBed experiments, and
in 16 of the experiments, rebar reduced standard
errors by 25-45\% relative to estimates without covariate adjustment.
However, in three experiments, the rebar estimates had higher standad
errors than their unadjusted counterparts (in the remaining three
experiments, the improvement was moderate).
In general, when $\predcxr$ is a poor
differs greatly from $\yci$, so that $(m_i-\hmi^{rebar})^2>m_i^2$, covariance
adjustment increases sampling variance.
This will be the case if an algorithm trained in the remnant
extrapolates poorly to the experimental sample---for instance, if the
distribution of $\bm{x}$, or the distribution of $\yc$
conditional on $\bm{x}$, differs subsantially between the two samples.
To make matters worse, the performance of $\predcr{\cdot}$ in the
experimental set---where it counts---may not be checked directly.
Once a researcher uses observed experimental outcomes $Y$ to select
$\predcr{\cdot}$, the resulting imputations $\predcxr$
will no longer be independent of $T$, violating
(\ref{eq:indPred}).

An additional problem with rebar is that, since typically only the
control condition is present in the remnant, $\predcr{\cdot}$ is used
to impute both potential outcomes as $\predcxr$.
This may further increase squared prediction error $(m_i-\hmi^{rebar})^2$.
Of course, a preliminary estimate of $\bar{\tau}$ is available from
the experimental data, but as before, incorporating experimental
outcomes into $\hat{m}$ induces a dependence between $\hat{m}$ and
$T$.

The remnant from an experiment is often much larger than the
experimental sample, and may provide fertile ground for imputing
potential outcomes, especially in the presence of rich
high-dimensional covariates.
However, absent methods to use experimental data to assess imputations'
accuracy and account for possible treatment effects, covariate
adjustment using the remnant is risky.
